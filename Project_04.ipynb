{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 4:  Web Scraping for Retail Arbitrage\n",
    "\n",
    "### Finding Underpriced RVs on Craigslist\n",
    "\n",
    "![](https://snag.gy/WrdUMx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will be practicing our web scraping skills.  You can use BeautifulSoup, Scrapy, Selenium or Python requests in order to complete this project.  \n",
    "\n",
    "> In order to run code from the command line, instead of the notebook, you just need to save your code to a file (with a .py extension), and run it using the Python interpreter:<br><br>\n",
    "> `python my_file.py`\n",
    "\n",
    "You will be building a process to scrape a single category of search results on Craigslist, that can easily be applied to other categories by changing the search terms.  The main goal is to be able to target and scrape a single page given a set of parameters.\n",
    "\n",
    "**If you use Scrapy, provide your code in a folder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your libraries for beautifulsoup, scrapy / requests / pandas / numpy / etc\n",
    "Setup whichever libraries you need. Review past material for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREPARE REQUIRED LIBRARIES\n",
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1(a)  Procure a list of the largest US cities from Wikipedia (non-exhaustive list)\n",
    "Search, research, and scrape Wikipedia for a list of the largest US cities.  There are a few sources but find one that is in a nice table.  We don't want all cities, just signifficant cities.  Examine your source.  Look for what can be differentiable.\n",
    "\n",
    "- Use requests\n",
    "- Build XPath query(ies)\n",
    "- Extract to a list\n",
    "- Clean your list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SCRAPE WIKIPEDIA FOR LARGEST US CITIES (NON-EXHAUSTIVE LIST)\n",
    "# USE https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
    "\n",
    "\n",
    "HTML = requests.get('https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population').text\n",
    "cities = Selector(text=HTML).xpath('//*[@id=\"mw-content-text\"]/table[4]//td[2]//text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unicode"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1(b) Clean Your List\n",
    "\n",
    "Optionally, filter out any cities with impropper ASCII characters.  A smaller list will be easier to look at.  However you may not need to filter these if you spend more time scraping a more concise city list.  This list should help you narrow down the list of regional Craigslist sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#THE LIST OF CITIES APPEAR AS STRINGS BUT I NEED TO CLEAN THE LIST.  TAKING THE LIST FROM WIKIPEDIA I HAVE NUMERIC \n",
    "#VALUES WHICH ACCOMPANY THE LIST OF CITIES.  \n",
    "\n",
    "for i in cities: \n",
    "    if i[0] == '[':  #CAN I WRITE THIS AS LAMBDA FUNCTION \n",
    "        cities.remove(i)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing the type of our file cities from unicode to str.  \n",
    "\n",
    "cities = [i.encode('ascii','ignore') for i in cities]\n",
    "type(cities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York',\n",
       " 'Los Angeles',\n",
       " 'Chicago',\n",
       " 'Houston',\n",
       " 'Philadelphia',\n",
       " 'Phoenix',\n",
       " 'San Antonio',\n",
       " 'San Diego',\n",
       " 'Dallas',\n",
       " 'Austin']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONLY RETAIN PROPERLY FORMED CITY NAMES\n",
    "cities[:100]\n",
    "del cities[9] #deleing the city san jose.  It doesnts have a craigslist page \n",
    "cities[:10] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2(a)  Write a function to capture current pricing information via Craigslist in one city.\n",
    "Choose a city from your scraped data, then go to the cooresponding city section on Craigslist, searching for \"rv\" in the auto section.  Write a method that pulls out the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9999.0,\n",
       " 19500.0,\n",
       " 19500.0,\n",
       " 20700.0,\n",
       " 20700.0,\n",
       " 2000.0,\n",
       " 20700.0,\n",
       " 20700.0,\n",
       " 20700.0,\n",
       " 20700.0,\n",
       " 20700.0,\n",
       " 2000.0,\n",
       " 850.0,\n",
       " 9999.0,\n",
       " 56000.0,\n",
       " 19500.0,\n",
       " 19500.0,\n",
       " 78000.0,\n",
       " 19999.0,\n",
       " 9999.0,\n",
       " 5900.0,\n",
       " 13750.0,\n",
       " 13000.0,\n",
       " 15995.0,\n",
       " 18750.0,\n",
       " 18400.0,\n",
       " 78000.0,\n",
       " 12000.0,\n",
       " 64900.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosing to look and compare Gulf Stream models beging with the cleveland\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fnc(str):\n",
    "    url = 'http://%s.craigslist.org/search/rva?query=gulf+stream'\n",
    "    HTML = requests.get(url%str.replace(' ', '').replace('.','')).text\n",
    "    RV = Selector(text=HTML).xpath('//*[@id=\"sortable-results\"]/div[1]/p/a/span/text()').extract()\n",
    "    return [float(i.encode('ascii', 'ignore').strip('$')) for i in RV] \n",
    "    \n",
    "fnc('new york')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Create a function that creates search string URLs to repeat the procedcure upon the list of cities obtained from Wikipedia cities\n",
    "\n",
    "Major US cities on Craigslist typically have their own corresponding section (ie: SFBay Area, NYC, Boston, Miami, Seattle, etc).  Later, you will use these to query search results for various metropolitian regions listed on Craigslist.  Between the major metropolitan Craigslist sites, the only thing that will differ is the URL's that correspond to them.\n",
    "\n",
    "The point of the \"mapping\":  Create a data structure that allows you to iterate with both the name of the city from Wikipedia, with the cooresponding variable that that will allow you to construct each craigslist URL for each region.\n",
    "\n",
    "> For San Francsico (the Bay Area metropolitan area), the url for the RV search result is:\n",
    "> http://sfbay.craigslist.org/search/sss?query=rv\n",
    ">\n",
    "> The convention is http://[region].craigslist.org/search/sss?query=rf\n",
    "> Replacing [region] with the corresponding city name will allow you to quickly iterate through each regional Craigslist site, and scrape the prices from the search results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prices for Gulf Streams in cleveland are [19500.0, 22995.0, 45000.0, 12000.0, 8500.0, 16488.0, 17000.0, 10500.0, 14500.0, 5495.0, 5995.0, 2700.0, 45000.0, 18995.0, 19800.0, 6000.0, 5495.0, 12995.0, 12000.0, 11000.0, 25000.0, 24000.0, 31900.0, 5995.0, 12995.0]\n"
     ]
    }
   ],
   "source": [
    "##AARON GAVE ME THIS HINT.  USING %S, STRING OPERAND I AM ABLE, GIVEN ANY INPUT CITY WITHIN CRAIGSLIST TO HEAD TO THE \n",
    "#RESPECTIVE CRAIGSLIST PAGE AND FINDE VALUES FOR ALL GULF STREAM RV'S. \n",
    "\n",
    "def fnc(city):\n",
    "    url = 'http://%s.craigslist.org/search/rva?query=gulf+stream'\n",
    "    HTML = requests.get(url%city).text\n",
    "    RV = Selector(text=HTML).xpath('//*[@id=\"sortable-results\"]/div[1]/p/a/span/text()').extract()\n",
    "    print 'The prices for Gulf Streams in', city, 'are',[float(i.encode('ascii', 'ignore').strip('$')) for i in RV]\n",
    "     \n",
    "\n",
    "fnc('cleveland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Define a function to caculate mean and median price per city.\n",
    "\n",
    "Now that you've created a list of cities you want to scrape, adapt your solution for grabbing data in one region site, to grab data for all regional sites that you collected, then calculate the mean and median price of RV results from each city.\n",
    "\n",
    "> Look at the URLs from a few different regions (ie: portland, phoenix, sfbay), and find what they have in common.  Determine the area in the URL string that needs to change the least, and figure out how to replace only that portion of the URL in order to iterate through each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average price for Gulf Streams sales in columbus is 18166.0322581 and the median price is 15500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18166.032258064515, 15500.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CREATING A FUNCTION TO COMPUTE THE MEAN AND MEDIAN OF THE VALUES \n",
    "\n",
    "def fnc_Mean_and_Median(city):\n",
    "    url = 'http://%s.craigslist.org/search/rva?query=gulf+stream'\n",
    "    HTML = requests.get(url%city).text\n",
    "    RV = Selector(text=HTML).xpath('//*[@id=\"sortable-results\"]/div[1]/p/a/span/text()').extract()\n",
    "    Mean = [float(i.encode('ascii', 'ignore').strip('$')) for i in RV]\n",
    "    Median = sorted([float(i.encode('ascii', 'ignore').strip('$')) for i in RV])\n",
    "    Average = np.mean(Mean)\n",
    "    Median = np.median(Median)\n",
    "    print 'The average price for Gulf Streams sales in',city,'is', Average, 'and the median price is', Median\n",
    "    \n",
    "    return Average, Median\n",
    "\n",
    "fnc_Mean_and_Median('columbus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Run your scraping process, and save your results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#I AM CREATING A FILE, FILE.CSV WHICH WILL CONTAIN THE FOLLOWING: ORIGIN, DESTINATION, DISTANCE BETWEEN THE TWO IN MILES\n",
    "#ESTIMATED FUEL COSTS IN US DOLLARS AND THE AVERAGE AND MEDIAN PRICES FOR GULF STREAM IN THE ORIGIN CITY.  THE FILE IS \n",
    "#IS CREATED BELOW AFTER I FINISH MY SCRAPE.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 6. Do an analysis of the RV market.  Are there any other variables you could pull out of the markup to help describe your dataset?\n",
    "\n",
    "Go head we'll wait.  Anything notable about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NOT SURE WHAT IS BEING ASKED HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7. Does it makes sense to buy RVs in one region and sell them in another?\n",
    "\n",
    "### Use the Google Distance API to inform your decision.\n",
    "\n",
    "Assuming the cost of shipping or driving from one regional market to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles, CA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 2,242 mi\n"
     ]
    }
   ],
   "source": [
    "#Adding my API key to the provided website my function here allows for input of any two cities, and produces their \n",
    "#distance.  \n",
    "def Google_API(origin, destination):\n",
    "    r = requests.get('https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins=%s&destinations=%s&key=AIzaSyC8n5AcTgesbLYrNRMgU_FQ1yuwNRP0kuc'%(origin, destination)).json()\n",
    "    print r['origin_addresses'][0].strip()\n",
    "    print r['destination_addresses'][0].strip()\n",
    "    print 'The distance between them is',r['rows'][0]['elements'][0]['distance']['text'].strip()  \n",
    "\n",
    "Google_API('Los Angeles', 'Columbus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the website eia.gov they have an average u.s. diesel fuel cost per gallon at 2.40.  Also, I decided to use a \n",
    "#pretty standard truck to pull our rv.  2016 Dodge Ram 3500 which has an average mpg of 14 miles/gallon. \n",
    "\n",
    "#So changing my function to calculate fuel costs and give the mean and median sales of the destination city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles, CA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 2,242 mi\n",
      "The estimated cost to haul an RV from Columbus to Los Angeles is 384.342857143\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Los Angeles', 'Columbus', 2242.0, 384.3428571428571)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Google_API2(origin, destination):\n",
    "    r = requests.get('https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins=%s&destinations=%s&key=AIzaSyC8n5AcTgesbLYrNRMgU_FQ1yuwNRP0kuc'%(origin, destination)).json()\n",
    "    print r['origin_addresses'][0].strip()\n",
    "    print r['destination_addresses'][0].strip()\n",
    "    print 'The distance between them is',r['rows'][0]['elements'][0]['distance']['text'].strip()\n",
    "    j = r['rows'][0]['elements'][0]['distance']['text'].strip()\n",
    "    dist = j.encode('ascii', 'ignore').split()\n",
    "    Dist = float(dist[0].replace(',',''))\n",
    "    Estimate = (Dist/14.0)*2.40 #DIST/CAR RATE * MILES/GALL\n",
    "    print 'The estimated cost to haul an RV from',destination,'to', origin,'is',Estimate\n",
    "    \n",
    "    \n",
    "    fnc_Mean_and_Median(destination)\n",
    "    \n",
    "    return origin, destination, Dist, Estimate\n",
    "Google_API2('Los Angeles', 'Columbus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York, NY, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 533 mi\n",
      "The estimated cost to haul an RV from Columbus to New York is 91.3714285714\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Los Angeles, CA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 2,242 mi\n",
      "The estimated cost to haul an RV from Columbus to Los Angeles is 384.342857143\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Chicago, IL, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 359 mi\n",
      "The estimated cost to haul an RV from Columbus to Chicago is 61.5428571429\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Houston, TX, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,155 mi\n",
      "The estimated cost to haul an RV from Columbus to Houston is 198.0\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Philadelphia, PA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 469 mi\n",
      "The estimated cost to haul an RV from Columbus to Philadelphia is 80.4\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Phoenix, AZ, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,874 mi\n",
      "The estimated cost to haul an RV from Columbus to Phoenix is 321.257142857\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "San Antonio, TX, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,313 mi\n",
      "The estimated cost to haul an RV from Columbus to San Antonio is 225.085714286\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "San Diego, CA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 2,226 mi\n",
      "The estimated cost to haul an RV from Columbus to San Diego is 381.6\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Dallas, TX, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,039 mi\n",
      "The estimated cost to haul an RV from Columbus to Dallas is 178.114285714\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Austin, TX, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,235 mi\n",
      "The estimated cost to haul an RV from Columbus to Austin is 211.714285714\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Jacksonville, FL, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 807 mi\n",
      "The estimated cost to haul an RV from Columbus to Jacksonville is 138.342857143\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "San Francisco, CA, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 2,434 mi\n",
      "The estimated cost to haul an RV from Columbus to San Francisco is 417.257142857\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Indianapolis, IN, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 175 mi\n",
      "The estimated cost to haul an RV from Columbus to Indianapolis is 30.0\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Columbus, OH, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1 ft\n",
      "The estimated cost to haul an RV from Columbus to Columbus is 0.171428571429\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n",
      "Fort Worth, TX, USA\n",
      "Columbus, OH, USA\n",
      "The distance between them is 1,071 mi\n",
      "The estimated cost to haul an RV from Columbus to Fort Worth is 183.6\n",
      "The average price for Gulf Streams sales in Columbus is 18166.0322581 and the median price is 15500.0\n"
     ]
    }
   ],
   "source": [
    "#CREATING A DATAFRAME WHICH TAKES, DESTINATION COLUMBUS WHICH IS SET, AND WILL TAKE ORIGIN CITY WHICH IS THE FIRST 10\n",
    "#IN MY CITIES LIST AND HOLDS THE DISTANCE FROM COLUMBUS, AND THE CALCUTED FUEL COST.  \n",
    "\n",
    "data = []\n",
    "\n",
    "for i in cities[:15]:\n",
    "    data.append(Google_API2(i, 'Columbus'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CONVERTING MY LIST OF DATA TO AN ACTUAL DATAFRAME\n",
    "My_List = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>Destin</th>\n",
       "      <th>Distance</th>\n",
       "      <th>FuelPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>533.0</td>\n",
       "      <td>91.371429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>2242.0</td>\n",
       "      <td>384.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chicago</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>359.0</td>\n",
       "      <td>61.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Houston</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>469.0</td>\n",
       "      <td>80.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>321.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1313.0</td>\n",
       "      <td>225.085714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>2226.0</td>\n",
       "      <td>381.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dallas</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1039.0</td>\n",
       "      <td>178.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Austin</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>211.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jacksonville</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>807.0</td>\n",
       "      <td>138.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>2434.0</td>\n",
       "      <td>417.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>175.0</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Columbus</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.171429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fort Worth</td>\n",
       "      <td>Columbus</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>183.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Origin    Destin  Distance   FuelPrice\n",
       "0        New York  Columbus     533.0   91.371429\n",
       "1     Los Angeles  Columbus    2242.0  384.342857\n",
       "2         Chicago  Columbus     359.0   61.542857\n",
       "3         Houston  Columbus    1155.0  198.000000\n",
       "4    Philadelphia  Columbus     469.0   80.400000\n",
       "5         Phoenix  Columbus    1874.0  321.257143\n",
       "6     San Antonio  Columbus    1313.0  225.085714\n",
       "7       San Diego  Columbus    2226.0  381.600000\n",
       "8          Dallas  Columbus    1039.0  178.114286\n",
       "9          Austin  Columbus    1235.0  211.714286\n",
       "10   Jacksonville  Columbus     807.0  138.342857\n",
       "11  San Francisco  Columbus    2434.0  417.257143\n",
       "12   Indianapolis  Columbus     175.0   30.000000\n",
       "13       Columbus  Columbus       1.0    0.171429\n",
       "14     Fort Worth  Columbus    1071.0  183.600000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHANGING MY COLUMNS TO THE FOLLOWING\n",
    "\n",
    "My_List.columns = ['Origin', 'Destin', 'Distance', 'FuelPrice']\n",
    "My_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "My_List.to_csv('My_Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 8.  Choose another area of Craigslist to scrape where retail arbitrage might be possible.\n",
    "\n",
    "#### Cross-reference possible opportunities with the eBay API Interface.\n",
    "\n",
    "**Choose an area having more than a single page of results, then scrape multiple regions, multiple pages of search results and or details pages.**\n",
    "\n",
    "This is the true exercise of being able to understand how to successfully plan, develop, and employ a broader scraping strategy.  Even though this seems like a challenging task, a few tweeks of your current code can make this very managable if you've pieced together all the touch points.  If you are still confused as to some of the milestones within this process, this is an excellent opportunity to round out your understanding, or help you build a list of questions to fill in your gaps.\n",
    "\n",
    "_Use Scrapy!  Provide your code in this project directory when you submit this project._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4.1: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few weeks ago we imputed age values in the titanic dataset. Now we're going to use those imputed values to predict survivorship using \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", and \"Fare\". Get the traning and testing data from here: https://www.kaggle.com/c/titanic/data (you will need to make an account). Copy your imputation code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import patsy\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.columns\n",
    "\n",
    "#first I am going to take the train set and categorize the Pclass, Parch and SibSp features.  I decided against setting\n",
    "#SibSp and Parch as categorical variables.  When running regression it was not able to predict when, due to \n",
    "#categorization, columns of full zeros were created. \n",
    "train_df['Pclass'] = train_df['Pclass'].astype('category')\n",
    "\n",
    "#within the Age columns/feature there are 177 null values I am going to fill in with the Average age with the Train set \n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I want to categorize the Sex column but first, changing the feature to single boolean rep. where male = 1 and female\n",
    "# =0.  \n",
    "\n",
    "def change_feature(x):\n",
    "    if x[0] == 'm':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "train_df['Sex'] = train_df['Sex'].apply(change_feature)\n",
    "train_df['Sex'] = train_df['Sex'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0]\n",
       "Categories (2, float64): [1.0, 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Age = train_df.Age.fillna(train_df.Age.mean())\n",
    "\n",
    "train_df.Age.isnull().sum()\n",
    "#just a quick check to verify here, but the NaN or null values within the Age column have been filled in with the \n",
    "#average age. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = test_df.fillna(test_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FIRST THING I DO WILL BE STANDARDIZING THE AGE AND FARE COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting both Age and Fare variables to the respective train_df columns \n",
    "Age = train_df.Age\n",
    "Fare = train_df.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler = preprocessing.StandardScaler().fit(Age)\n",
    "train_df.Age = scaler.transform(Age)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(Fare)\n",
    "train_df.Fare = scaler.transform(Fare)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Here I am settting up the features asked to consider for our knn regression and creating two new data frames, knn_Train\n",
    "#and knn_Test with only the feautres Parch, Sex, Age, SibSp, and Fare.  Also, categoriizng the Sex column, running \n",
    "#the function change_feature(), created above to change the column from males/females to 1/0.\n",
    "\n",
    "knn_Train = train_df[['Parch', 'Sex', 'Age', 'SibSp', 'Fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "knn_Test = test_df[['Parch', 'Sex', 'Age', 'SibSp', 'Fare']]\n",
    "knn_Test['Sex'] = knn_Test['Sex'].apply(change_feature)\n",
    "knn_Test['Sex'] = knn_Test['Sex'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Predict using kNN with k=1,2,3,4,5,6,7,8,9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "prediction = []\n",
    "#Creating prediction list which will hold the accuracy of each kNN with k = 1-9 \n",
    "for i in range(1,10): \n",
    "    \n",
    "    kNN_clf = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    kNN_clf.fit(knn_Train, train_df['Survived'])\n",
    "    kNN_clf.predict(knn_Test)\n",
    "    Acc_Score = kNN_clf.score(knn_Train, train_df['Survived']) \n",
    "    \n",
    "    prediction.append(Acc_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97979797979797978,\n",
       " 0.87991021324354657,\n",
       " 0.877665544332211,\n",
       " 0.85746352413019078,\n",
       " 0.85185185185185186,\n",
       " 0.83950617283950613,\n",
       " 0.8271604938271605,\n",
       " 0.83164983164983164,\n",
       " 0.82940516273849607]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THE ACCURACY SCORES FOR EACH K\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when k = 1 highest accuracy is 0.979797979798\n"
     ]
    }
   ],
   "source": [
    "print 'when k = 1 highest accuracy is', np.max(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_df[['Parch','Fare', 'SibSp', 'Age', 'Sex']].values, \n",
    "                                                    train_df['Survived'].values, \n",
    "                                                    test_size=0.33, stratify=train_df['Survived'].values, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=77)\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = [\"C(Pclass)\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "y_train, X_train = patsy.dmatrices('Survived ~'+'+'.join(feats), \n",
    "                       data=train_df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test, X_test = patsy.dmatrices('1 ~'+'+'.join(feats), \n",
    "                       data=test_df, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df['Predict'] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_predic(x):\n",
    "    if x>0.32:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['Survived'] = test_df['Predict'].apply(make_predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('my_first_kaggle.csv',\n",
    "              columns=['PassengerId', 'Survived'], \n",
    "              index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KAGGLE USSERNAME: Punkymonkey88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
